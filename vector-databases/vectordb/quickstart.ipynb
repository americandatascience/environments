{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e07b06-7463-4c51-8816-5138f5a48688",
   "metadata": {},
   "source": [
    "# Chroma Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e2ae6-fc57-4b5f-891e-995ec801b887",
   "metadata": {},
   "source": [
    "## 1. Load API Key with .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0d216-bf3c-4bb7-ad7c-24d82cfddfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Your .env file can contain connection env vars for Chroma, but this notebook example uses ephemeral local storage so this technically isn't necessary\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea8692-dd18-4005-8f7d-aafa67e2924e",
   "metadata": {},
   "source": [
    "## 2. Initialize Chroma client\n",
    "\n",
    "Next, use your API key to initialize your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fbd0a-b230-4a28-bc04-29885ba30655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e09a3-bc10-485c-9c22-12ab66519bda",
   "metadata": {},
   "source": [
    "## 3. Prepare language model for vector encoder\n",
    "\n",
    "We use a small transformers language model to create 364-dimensional embeddings. You can out models for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1712bf-4dbb-447d-ab98-6f7b9a3f4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = embedder.encode(\"Example sentences.\")\n",
    "dimension = embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a64b0-bab8-4ad0-8bea-9b259e90b10e",
   "metadata": {},
   "source": [
    "## 4. Create a Chroma collection\n",
    "\n",
    "This creates a collection named \"quickstart\" that performs similarity search with your vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da8c74-9a73-4682-8182-458187caef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"quickstart\"\n",
    "collection = client.create_collection(collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab7ee6-33fa-44df-bccd-6a60c83569fd",
   "metadata": {},
   "source": [
    "## 5. Generate vector values from wikipedia text\n",
    "\n",
    "We retrieve a wikipedia based dataset with Hugging Face's datasets library. Note that this dataset contains Cohere's vectors, but we're generating our own in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fd4c0-96af-43ef-9a0d-d58e464cc84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import cohere\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "max_docs = 1000 # Increase to use etnire dataset\n",
    "docs_stream = load_dataset(f\"Cohere/wikipedia-22-12-simple-embeddings\", split=\"train\", streaming=True)\n",
    "\n",
    "documents = []\n",
    "embeddings = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "\n",
    "pbar = tqdm(total=max_docs)\n",
    "for doc in docs_stream:\n",
    "    documents.append(doc[\"title\"] + \" \" + doc[\"text\"])\n",
    "    embeddings.append(embedder.encode(doc[\"title\"] + \" \" + doc[\"text\"]).tolist())\n",
    "    metadatas.append({\n",
    "        \"title\": doc[\"title\"],\n",
    "        \"text\": doc[\"text\"],\n",
    "        \"url\": doc[\"url\"],\n",
    "    })\n",
    "    ids.append(str(doc[\"id\"]))\n",
    "    pbar.update(1)\n",
    "    if len(embeddings) >= max_docs:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503e499-e141-467b-a402-ef7bf4a5aa64",
   "metadata": {},
   "source": [
    "## 6. Upsert vectors\n",
    "\n",
    "Now that youâ€™ve created your collection and the vector embeddings of your wikipedia data, you can upsert these vectors into your collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950cc29e-28b9-4df8-981a-e454dcb4af5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de54d2-b352-4078-8162-11a6af1252aa",
   "metadata": {},
   "source": [
    "## 7. Check the that vectors were inserted to che collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654bb6a-2382-4f82-8eb2-6bc8e6642add",
   "metadata": {},
   "source": [
    "## 8. Run a similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a6e27-a400-4e61-a606-6bf7346ec720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"What's the greatest human advancement of all time?\"\n",
    "\n",
    "def search(query: str):\n",
    "    query_results = collection.query(\n",
    "        query_embeddings=[embedder.encode(query).tolist()],\n",
    "        include=[\"documents\"]\n",
    "    )\n",
    "    matches = query_results.get(\"documents\", [])\n",
    "    if matches:\n",
    "        return matches[0][0]\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa01708-26b5-434c-bc3b-5debf204c6ea",
   "metadata": {},
   "source": [
    "## 9. Deploy an app to port forward and share publically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301fcbf-8ad1-4858-9e8f-9d5cd3e095ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=search,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your query here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Search Wikipedia text with a Chroma Collection\",\n",
    "    description=\"This is a simple wikipedia search engine powered by transformers and chroma\",\n",
    ")\n",
    "\n",
    "# Run the Gradio app on localhost:5000 or whichever port you specified\n",
    "iface.launch(server_port=5000, inline=False, quiet=True)\n",
    "\n",
    "print(f\"See your app deployed publically with the port you're securely forwarding: {os.environ.get('PORT_FORWARD_URL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c80513-f34b-4a9d-b79f-76925380fd2d",
   "metadata": {},
   "source": [
    "## 10. Clean up\n",
    "\n",
    "When you no longer need the collection, call `drop_collection` and specify the name to shut it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920f6fe-3df3-44d9-8a47-0ceeace8a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_collection(\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3e397-1389-47ee-91c5-58b6d17cb7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
